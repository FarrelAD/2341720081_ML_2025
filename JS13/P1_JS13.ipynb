{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518ba6fd",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02b9b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a04c70",
   "metadata": {},
   "source": [
    "# Create Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58afed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee9813",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcedbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# weight initialization\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f1d8b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a7a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2608909593785421\n",
      "Epoch 1000, Loss: 0.24683178250881707\n",
      "Epoch 2000, Loss: 0.2139044926052045\n",
      "Epoch 3000, Loss: 0.06664500161583195\n",
      "Epoch 4000, Loss: 0.01795358926514981\n",
      "Epoch 5000, Loss: 0.00904758716253779\n",
      "Epoch 6000, Loss: 0.0058296922966290295\n",
      "Epoch 7000, Loss: 0.004235207953931878\n",
      "Epoch 8000, Loss: 0.003299451562356559\n",
      "Epoch 9000, Loss: 0.002689769228950644\n",
      "Prediksi:\n",
      "[[0.05264851]\n",
      " [0.95527302]\n",
      " [0.95404956]\n",
      " [0.0465971 ]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    error = y - a2\n",
    "\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a9f8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf25655",
   "metadata": {},
   "source": [
    "1. Ubah jumlah neuron hidden layer menjadi 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31fc28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = 2\n",
    "hidden_size = 3 # update to 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# weight initialization\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45391ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.26507900768373116\n",
      "Epoch 1000, Loss: 0.23119023576726516\n",
      "Epoch 2000, Loss: 0.17480857152754764\n",
      "Epoch 3000, Loss: 0.10279618056304898\n",
      "Epoch 4000, Loss: 0.03590326203395383\n",
      "Epoch 5000, Loss: 0.014934355301542326\n",
      "Epoch 6000, Loss: 0.008456462493709198\n",
      "Epoch 7000, Loss: 0.005678249059083687\n",
      "Epoch 8000, Loss: 0.004201487640187\n",
      "Epoch 9000, Loss: 0.003303913925825174\n",
      "Prediksi:\n",
      "[[0.04259867]\n",
      " [0.93582872]\n",
      " [0.95856948]\n",
      " [0.05641452]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    error = y - a2\n",
    "\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a1e73",
   "metadata": {},
   "source": [
    "2. Bandingkan hasil loss dengan konfigurasi awal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac73f3b",
   "metadata": {},
   "source": [
    "**Jawab:** Dengan menambahkan 1 neuron tambahan pada hidden layer, dapat menghasilkan loss yang sedikit lebih tinggi pada akhir epoch ($0.00330$ berbanding $0.00268$). Namun, dari sisi kurva loss yang diperoleh, penambahan neuron baru dapat menghasilkan kurva yang lebih *smooth*, proses penurunan loss bisa terjadi secara perlahan. Ini biasanya karena dengan semakin bertambahnya neuron, maka bisa memungkinkan proses yang lebih stabil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0fd80",
   "metadata": {},
   "source": [
    "3. Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a282f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be3d1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = 2\n",
    "hidden_size = 3 # update to 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# weight initialization\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63cf59b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.26059201168945956\n",
      "Epoch 1000, Loss: 0.16708116997603079\n",
      "Epoch 2000, Loss: 0.16682308662785683\n",
      "Epoch 3000, Loss: 0.16678087738240846\n",
      "Epoch 4000, Loss: 0.16674646799959453\n",
      "Epoch 5000, Loss: 0.16671999105005358\n",
      "Epoch 6000, Loss: 0.1667038903333133\n",
      "Epoch 7000, Loss: 0.1666994448476091\n",
      "Epoch 8000, Loss: 0.16670052752500733\n",
      "Epoch 9000, Loss: 0.16669557391409204\n",
      "Prediksi:\n",
      "[[0.66650949]\n",
      " [0.66650949]\n",
      " [0.66650949]\n",
      " [0.01049776]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    # forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1) # perubahan dari sigmoid function menjadi relu\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    error = y - a2\n",
    "\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(a1) # perubahan menjadi relu derivative\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c63511",
   "metadata": {},
   "source": [
    "Hasil yang diperoleh cenderung lebih buruk dari metode sebelumnya. Loss yang didapat cenderung stuck di sekitar $0.166...$. Hal ini dikarenakan data yang ada (XOR) cenderung tidak terpisah secara linear. Sedangkan ReLU cenderung lebih cocok pada data yang memiliki sifat lebih linear. Jika neural network tidak memiliki cukup neuron, maka dalam hal ini, hasilnya bisa lebih buruk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gideon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
